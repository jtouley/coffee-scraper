import requests
from bs4 import BeautifulSoup

# Function to scrape individual coffee pages
def scrape_coffee_page(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        # Extract desired data from the page
        coffee_data = {
            'name': soup.find('h1', class_='coffee-name').text,
            'description': soup.find('div', class_='coffee-description').text
            # Add more fields as needed
        }
        return coffee_data
    else:
        print(f"Failed to fetch page: {url}")
        return None

# Main scraping function
def scrape_main_page(url_main, total_pages):
    all_coffee_data = []
    for page in range(1, total_pages + 1):
        url = f"{url_main}?page={page}"
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # Extract URLs of individual coffee pages
            coffee_urls = [a['href'] for a in soup.find_all('a', class_='coffee-link')]
            # Loop through individual coffee pages and scrape data
            for coffee_url in coffee_urls:
                coffee_data = scrape_coffee_page(coffee_url)
                if coffee_data:
                    all_coffee_data.append(coffee_data)
            # Print progress
            if (page % 25) == 0:
                print(f'Scraped {page} pages')
        else:
            print(f"Failed to fetch page: {url}")
    
    return all_coffee_data

# Example usage
main_page_url = "https://onyxcoffeelab.com/collections/coffee"
total_pages_to_scrape = 100
print('Begin scraping...')
coffee_data = scrape_main_page(main_page_url, total_pages_to_scrape)
print('Done scraping.')
print(f'Total coffee data scraped: {len(coffee_data)}')
